{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9962b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import main_classificaiton as mc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b8af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c4dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. w/o data representation & RNN model \n",
    "config1 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'RNN', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC_layer'} 중 택 1\n",
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'num_layers' : 2, # recurrnet layers의 수, Default : 1\n",
    "            'hidden_size' : 64, # hidden state의 벡터차원 수\n",
    "            'attention' : False, # True일 경우 attention layer를 추가\n",
    "            'dropout' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            'bidirectional' : True, # 모델의 양방향성 여부\n",
    "            'batch_size' : 64, #batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
    "# Case 2. w/o data representation &LSTM model \n",
    "config2 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'LSTM', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC_layer'} 중 택 1\n",
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'num_layers' : 2, # recurrnet layers의 수, Default : 1\n",
    "            'hidden_size' : 64, # hidden state의 벡터차원 수\n",
    "            'attention' : False, # True일 경우 attention layer를 추가\n",
    "            'dropout' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            'bidirectional' : True, # 모델의 양방향성 여부\n",
    "            'batch_size' : 64, #batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
    "# Case 3. w/o data representation & GRU model \n",
    "config3 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'GRU', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC_layer'} 중 택 1\n",
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'num_layers' : 2, # recurrnet layers의 수, Default : 1\n",
    "            'hidden_size' : 64, # hidden state의 벡터차원 수\n",
    "            'attention' : False, # True일 경우 attention layer를 추가\n",
    "            'dropout' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            'bidirectional' : True, # 모델의 양방향성 여부\n",
    "            'batch_size' : 64, #batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
    "# Case 4. w/o data representation & CNN_1D model \n",
    "config4 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'CNN_1D', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC_layer'} 중 택 1\n",
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'output_channels' : 64, # convolution channel size of output\n",
    "            'drop_out' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            'kernel_size' : 3, # convolutional filter size\n",
    "            'stride' : 1, # stride of the convolution. Default = 1 \n",
    "            'padding' : 0, # padding added to both sides of the input. Default = 0\n",
    "            'batch_size' : 64, # batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
    "# Case 5. w data representation & fully-connected layers \n",
    "# 특징 벡터의 사이즈 = 20 이라고 가정\n",
    "config5 = {\n",
    "        'with_representation': True, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'FC', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC'} 중 택 1\n",
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'input_representation' : 0, # 예를 들면 (80, 20) 차원의 벡터 (80은 window_length에 따른 관측치 수, 20은 representation 특징벡터 차원 수)를 넣어야 함. 지금은 loader부분에서 random값들어가 있음\n",
    "            'drop_out' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            'batch_size' : 64, # batch size\n",
    "            'bias': True, # bias [True, False]\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e84de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 9, 128)\n",
      "(7352,)\n",
      "(2947, 9, 128)\n",
      "(2947,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "train_x = pickle.load(open(os.path.join(data_dir, 'X_train.pkl'), 'rb'))\n",
    "train_y = pickle.load(open(os.path.join(data_dir, 'y_train.pkl'), 'rb'))\n",
    "test_x =  pickle.load(open(os.path.join(data_dir, 'X_test.pkl'), 'rb'))\n",
    "test_y = pickle.load(open(os.path.join(data_dir, 'y_test.pkl'), 'rb'))\n",
    "\n",
    "train_data = {'x' : train_x, 'y' : train_y}\n",
    "test_data = {'x' : test_x, 'y' : test_y}\n",
    "\n",
    "print(train_x.shape)  #shape : (num_of_instance x input_dims x window_size) = (7352, 9, 128)\n",
    "print(train_y.shape) #shape : (num_of_instance x input_dims x window_size) = (7352, )\n",
    "print(test_x.shape)  #shape : (num_of_instance x input_dims x window_size) = (2947, 9, 128)\n",
    "print(test_y.shape)  #shape : (num_of_instance x input_dims x window_size) = (2947)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564b0770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
      "train Loss: 1.6574 Acc: 0.3578\n",
      "val Loss: 1.4503 Acc: 0.3848\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.0065 Acc: 0.5565\n",
      "val Loss: 1.0774 Acc: 0.5391\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.9592 Acc: 0.5877\n",
      "val Loss: 1.0034 Acc: 0.5670\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.9334 Acc: 0.6057\n",
      "val Loss: 1.0058 Acc: 0.5778\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.8719 Acc: 0.6354\n",
      "val Loss: 0.9964 Acc: 0.6213\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.6879 Acc: 0.7171\n",
      "val Loss: 0.7186 Acc: 0.7458\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.5052 Acc: 0.8101\n",
      "val Loss: 0.5891 Acc: 0.8212\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.4008 Acc: 0.8599\n",
      "val Loss: 0.5279 Acc: 0.8484\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.3466 Acc: 0.8808\n",
      "val Loss: 0.4705 Acc: 0.8627\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.3060 Acc: 0.8973\n",
      "val Loss: 0.3717 Acc: 0.8906\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.2752 Acc: 0.9112\n",
      "val Loss: 0.3496 Acc: 0.9130\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.2896 Acc: 0.8980\n",
      "val Loss: 0.2873 Acc: 0.9211\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.2468 Acc: 0.9223\n",
      "val Loss: 0.3164 Acc: 0.9103\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.2346 Acc: 0.9257\n",
      "val Loss: 0.2714 Acc: 0.9279\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.2216 Acc: 0.9286\n",
      "val Loss: 0.3622 Acc: 0.8933\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.2056 Acc: 0.9299\n",
      "val Loss: 0.2824 Acc: 0.9171\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.1880 Acc: 0.9371\n",
      "val Loss: 0.2582 Acc: 0.9266\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.1922 Acc: 0.9362\n",
      "val Loss: 0.2600 Acc: 0.9245\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.2443 Acc: 0.9158\n",
      "val Loss: 0.3305 Acc: 0.8906\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.1791 Acc: 0.9408\n",
      "val Loss: 0.2492 Acc: 0.9293\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.1756 Acc: 0.9398\n",
      "val Loss: 0.2403 Acc: 0.9320\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.2708 Acc: 0.9116\n",
      "val Loss: 0.3683 Acc: 0.9048\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.1645 Acc: 0.9444\n",
      "val Loss: 0.2388 Acc: 0.9286\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.1549 Acc: 0.9476\n",
      "val Loss: 0.2439 Acc: 0.9300\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.1893 Acc: 0.9357\n",
      "val Loss: 0.2308 Acc: 0.9239\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.1640 Acc: 0.9417\n",
      "val Loss: 0.2323 Acc: 0.9341\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.1593 Acc: 0.9432\n",
      "val Loss: 0.2359 Acc: 0.9347\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.1837 Acc: 0.9371\n",
      "val Loss: 0.2484 Acc: 0.9164\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.1420 Acc: 0.9500\n",
      "val Loss: 0.2449 Acc: 0.9307\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.1551 Acc: 0.9458\n",
      "val Loss: 0.2475 Acc: 0.9225\n",
      "Training complete in 3m 17s\n",
      "Best val Acc: 0.936098\n",
      "[4 4 4 4 4]\n",
      "[[4.5214836e-03 1.5004937e-04 3.3503795e-05 8.2635731e-03 9.8703116e-01\n",
      "  2.0977977e-07]\n",
      " [4.9505834e-03 1.5442492e-04 3.3928223e-05 6.8437709e-03 9.8801708e-01\n",
      "  1.7939905e-07]\n",
      " [6.3765557e-03 1.7957440e-04 3.8538466e-05 5.6916475e-03 9.8771358e-01\n",
      "  1.7165915e-07]\n",
      " [5.4079797e-03 1.4515381e-04 3.6949557e-05 5.5562123e-03 9.8885357e-01\n",
      "  1.3897717e-07]\n",
      " [5.1595904e-03 1.3919613e-04 3.6768564e-05 5.7154642e-03 9.8894894e-01\n",
      "  1.4125800e-07]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1. w/o data representation & RNN\n",
    "config = config1\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
    "print(prob[:5]) # shape : (2947, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eefa80c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
      "train Loss: 1.7734 Acc: 0.2931\n",
      "val Loss: 1.7412 Acc: 0.3018\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.0326 Acc: 0.5496\n",
      "val Loss: 1.0919 Acc: 0.5534\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.9024 Acc: 0.6183\n",
      "val Loss: 0.9183 Acc: 0.6649\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.9062 Acc: 0.6169\n",
      "val Loss: 0.8528 Acc: 0.6880\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.7794 Acc: 0.6883\n",
      "val Loss: 0.8394 Acc: 0.7124\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.7871 Acc: 0.6541\n",
      "val Loss: 0.9775 Acc: 0.5846\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.7142 Acc: 0.6864\n",
      "val Loss: 0.7888 Acc: 0.6886\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.6371 Acc: 0.7419\n",
      "val Loss: 0.6810 Acc: 0.7444\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.6682 Acc: 0.7421\n",
      "val Loss: 0.7087 Acc: 0.7315\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.5376 Acc: 0.7832\n",
      "val Loss: 0.6536 Acc: 0.7322\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.5593 Acc: 0.7653\n",
      "val Loss: 0.5823 Acc: 0.7587\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.4992 Acc: 0.7890\n",
      "val Loss: 0.5864 Acc: 0.7723\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.4589 Acc: 0.8147\n",
      "val Loss: 0.5428 Acc: 0.7899\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.5841 Acc: 0.7633\n",
      "val Loss: 0.6265 Acc: 0.7661\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.9747 Acc: 0.6048\n",
      "val Loss: 0.9504 Acc: 0.6601\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.8185 Acc: 0.6390\n",
      "val Loss: 0.7326 Acc: 0.6751\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 1.0357 Acc: 0.5724\n",
      "val Loss: 1.0632 Acc: 0.5527\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.7707 Acc: 0.6402\n",
      "val Loss: 0.8157 Acc: 0.6649\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.7273 Acc: 0.6669\n",
      "val Loss: 0.7774 Acc: 0.6703\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.7370 Acc: 0.6643\n",
      "val Loss: 0.7219 Acc: 0.6914\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.5829 Acc: 0.7456\n",
      "val Loss: 0.5626 Acc: 0.7621\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.4625 Acc: 0.8065\n",
      "val Loss: 0.4900 Acc: 0.8063\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.5061 Acc: 0.7869\n",
      "val Loss: 0.5240 Acc: 0.7913\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.4248 Acc: 0.8327\n",
      "val Loss: 0.4651 Acc: 0.8300\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.5297 Acc: 0.7728\n",
      "val Loss: 0.5007 Acc: 0.7899\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.3707 Acc: 0.8628\n",
      "val Loss: 0.4090 Acc: 0.8491\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.3227 Acc: 0.8825\n",
      "val Loss: 0.4105 Acc: 0.8443\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.2921 Acc: 0.8924\n",
      "val Loss: 0.4024 Acc: 0.8457\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.2527 Acc: 0.9080\n",
      "val Loss: 0.3524 Acc: 0.8770\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.3912 Acc: 0.8585\n",
      "val Loss: 0.6833 Acc: 0.7158\n",
      "Training complete in 5m 9s\n",
      "Best val Acc: 0.903467\n",
      "[4 4 4 4 4]\n",
      "[[4.08150628e-02 4.82685026e-03 6.05355417e-05 2.26821308e-03\n",
      "  9.51980114e-01 4.92620056e-05]\n",
      " [1.84092752e-03 1.73389984e-04 6.27489862e-06 5.95301064e-03\n",
      "  9.92014647e-01 1.17181808e-05]\n",
      " [8.86968337e-04 7.29505919e-05 1.30653461e-05 1.46933170e-02\n",
      "  9.84327614e-01 6.11029827e-06]\n",
      " [7.98552763e-04 8.37308253e-05 8.70966414e-06 1.47835640e-02\n",
      "  9.84318018e-01 7.53098993e-06]\n",
      " [9.06702480e-04 7.17921066e-05 1.28198189e-05 1.35646695e-02\n",
      "  9.85438228e-01 5.87854493e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Case 2. w/o data representation & LSTM\n",
    "config = config2\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
    "print(prob[:5]) # shape : (2947, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7674f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
      "train Loss: 1.7511 Acc: 0.3561\n",
      "val Loss: 1.6960 Acc: 0.4018\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 0.9861 Acc: 0.5676\n",
      "val Loss: 1.1365 Acc: 0.5350\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.8708 Acc: 0.6320\n",
      "val Loss: 0.9467 Acc: 0.6383\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.6686 Acc: 0.7653\n",
      "val Loss: 0.6364 Acc: 0.8144\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.3027 Acc: 0.9106\n",
      "val Loss: 0.3224 Acc: 0.9300\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.1912 Acc: 0.9401\n",
      "val Loss: 0.2573 Acc: 0.9395\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.1561 Acc: 0.9471\n",
      "val Loss: 0.2446 Acc: 0.9388\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.1395 Acc: 0.9480\n",
      "val Loss: 0.2414 Acc: 0.9402\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.1350 Acc: 0.9536\n",
      "val Loss: 0.2387 Acc: 0.9341\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.1310 Acc: 0.9480\n",
      "val Loss: 0.2515 Acc: 0.9402\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.1219 Acc: 0.9534\n",
      "val Loss: 0.2522 Acc: 0.9395\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.1228 Acc: 0.9539\n",
      "val Loss: 0.2619 Acc: 0.9368\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.1180 Acc: 0.9539\n",
      "val Loss: 0.2590 Acc: 0.9375\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.1070 Acc: 0.9543\n",
      "val Loss: 0.2559 Acc: 0.9388\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.1031 Acc: 0.9580\n",
      "val Loss: 0.2562 Acc: 0.9375\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.0993 Acc: 0.9616\n",
      "val Loss: 0.2511 Acc: 0.9395\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.0993 Acc: 0.9590\n",
      "val Loss: 0.2497 Acc: 0.9354\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.0993 Acc: 0.9599\n",
      "val Loss: 0.2479 Acc: 0.9347\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.0949 Acc: 0.9629\n",
      "val Loss: 0.2530 Acc: 0.9375\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.0930 Acc: 0.9617\n",
      "val Loss: 0.2526 Acc: 0.9402\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.0895 Acc: 0.9633\n",
      "val Loss: 0.2572 Acc: 0.9395\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.1033 Acc: 0.9594\n",
      "val Loss: 0.2587 Acc: 0.9381\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.0879 Acc: 0.9628\n",
      "val Loss: 0.2631 Acc: 0.9375\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.0872 Acc: 0.9633\n",
      "val Loss: 0.2622 Acc: 0.9368\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.0851 Acc: 0.9628\n",
      "val Loss: 0.2667 Acc: 0.9368\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.0843 Acc: 0.9640\n",
      "val Loss: 0.2664 Acc: 0.9381\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.0870 Acc: 0.9623\n",
      "val Loss: 0.2735 Acc: 0.9375\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.0899 Acc: 0.9592\n",
      "val Loss: 0.2725 Acc: 0.9375\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0835 Acc: 0.9611\n",
      "val Loss: 0.2707 Acc: 0.9381\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0816 Acc: 0.9643\n",
      "val Loss: 0.2712 Acc: 0.9368\n",
      "Training complete in 4m 19s\n",
      "Best val Acc: 0.940857\n",
      "[4 4 4 4 4]\n",
      "[[1.4380289e-02 1.8129022e-03 2.0217443e-05 1.0951811e-03 9.8266625e-01\n",
      "  2.5202655e-05]\n",
      " [1.3365337e-02 1.9776577e-03 1.9504199e-05 1.0490593e-03 9.8355931e-01\n",
      "  2.9114295e-05]\n",
      " [1.3895592e-02 2.0615186e-03 2.0235577e-05 1.0369056e-03 9.8295593e-01\n",
      "  2.9711655e-05]\n",
      " [1.4425054e-02 2.1313692e-03 2.0490825e-05 1.0150188e-03 9.8237765e-01\n",
      "  3.0370185e-05]\n",
      " [1.4831049e-02 2.2043064e-03 2.0943333e-05 9.9724007e-04 9.8191512e-01\n",
      "  3.1408563e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Case 3. w/o data representation & GRU\n",
    "config = config3\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
    "print(prob[:5]) # shape : (2947, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59275e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
      "train Loss: 1.6331 Acc: 0.4045\n",
      "val Loss: 1.3934 Acc: 0.5398\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 0.5212 Acc: 0.7987\n",
      "val Loss: 0.6720 Acc: 0.8056\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.3854 Acc: 0.8604\n",
      "val Loss: 0.5499 Acc: 0.8368\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.3060 Acc: 0.8927\n",
      "val Loss: 0.4675 Acc: 0.8532\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.2494 Acc: 0.9141\n",
      "val Loss: 0.4058 Acc: 0.8783\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.2069 Acc: 0.9306\n",
      "val Loss: 0.3791 Acc: 0.8865\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.1786 Acc: 0.9386\n",
      "val Loss: 0.3540 Acc: 0.8967\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.1568 Acc: 0.9466\n",
      "val Loss: 0.3402 Acc: 0.8994\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.1403 Acc: 0.9522\n",
      "val Loss: 0.3284 Acc: 0.9055\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.1274 Acc: 0.9558\n",
      "val Loss: 0.3223 Acc: 0.9103\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.1179 Acc: 0.9578\n",
      "val Loss: 0.2973 Acc: 0.9205\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.1107 Acc: 0.9595\n",
      "val Loss: 0.2925 Acc: 0.9218\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.1022 Acc: 0.9606\n",
      "val Loss: 0.2865 Acc: 0.9211\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.0969 Acc: 0.9628\n",
      "val Loss: 0.2821 Acc: 0.9232\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.0933 Acc: 0.9628\n",
      "val Loss: 0.2863 Acc: 0.9211\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.0878 Acc: 0.9641\n",
      "val Loss: 0.2839 Acc: 0.9225\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.0840 Acc: 0.9641\n",
      "val Loss: 0.2888 Acc: 0.9259\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.0804 Acc: 0.9655\n",
      "val Loss: 0.2895 Acc: 0.9259\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.0784 Acc: 0.9658\n",
      "val Loss: 0.2922 Acc: 0.9252\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.0755 Acc: 0.9662\n",
      "val Loss: 0.2954 Acc: 0.9225\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.0736 Acc: 0.9670\n",
      "val Loss: 0.2986 Acc: 0.9361\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.0742 Acc: 0.9665\n",
      "val Loss: 0.2828 Acc: 0.9252\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.0698 Acc: 0.9697\n",
      "val Loss: 0.2897 Acc: 0.9245\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.0661 Acc: 0.9702\n",
      "val Loss: 0.2840 Acc: 0.9239\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.0652 Acc: 0.9691\n",
      "val Loss: 0.3002 Acc: 0.9232\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.0636 Acc: 0.9702\n",
      "val Loss: 0.3010 Acc: 0.9232\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.0615 Acc: 0.9702\n",
      "val Loss: 0.2938 Acc: 0.9239\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.0605 Acc: 0.9716\n",
      "val Loss: 0.2964 Acc: 0.9286\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0588 Acc: 0.9731\n",
      "val Loss: 0.2889 Acc: 0.9259\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0582 Acc: 0.9735\n",
      "val Loss: 0.2949 Acc: 0.9252\n",
      "Training complete in 0m 46s\n",
      "Best val Acc: 0.940177\n",
      "[4 4 4 4 4]\n",
      "[[4.9122332e-06 2.8034972e-05 2.2955086e-13 1.8831897e-05 9.9994826e-01\n",
      "  6.8109113e-14]\n",
      " [1.2523996e-05 3.6045538e-05 1.7067246e-13 8.2466970e-05 9.9986899e-01\n",
      "  4.1336472e-14]\n",
      " [8.5092443e-06 6.2387058e-05 2.3781855e-13 7.4034549e-05 9.9985504e-01\n",
      "  7.9870707e-14]\n",
      " [4.6790915e-06 8.4736566e-05 2.0288286e-13 5.2684180e-05 9.9985790e-01\n",
      "  1.2451327e-13]\n",
      " [1.8034656e-06 5.6174038e-05 8.0126525e-14 3.2506639e-05 9.9990952e-01\n",
      "  9.2559898e-14]]\n"
     ]
    }
   ],
   "source": [
    "# Case 4. w/o data representation & CNN_1D\n",
    "config = config4\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
    "print(prob[:5]) # shape : (2947, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b370aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
      "train Loss: 1.8034 Acc: 0.1852\n",
      "val Loss: 1.7961 Acc: 0.1869\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.7933 Acc: 0.1818\n",
      "val Loss: 1.7908 Acc: 0.1788\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 1.7917 Acc: 0.1823\n",
      "val Loss: 1.7904 Acc: 0.1788\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 1.7921 Acc: 0.1777\n",
      "val Loss: 1.7905 Acc: 0.1754\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 1.7912 Acc: 0.1877\n",
      "val Loss: 1.7905 Acc: 0.1727\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 1.7917 Acc: 0.1758\n",
      "val Loss: 1.7906 Acc: 0.1740\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 1.7906 Acc: 0.1785\n",
      "val Loss: 1.7909 Acc: 0.1761\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 1.7915 Acc: 0.1816\n",
      "val Loss: 1.7910 Acc: 0.1768\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 1.7904 Acc: 0.1787\n",
      "val Loss: 1.7911 Acc: 0.1774\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 1.7907 Acc: 0.1809\n",
      "val Loss: 1.7911 Acc: 0.1768\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 1.7910 Acc: 0.1760\n",
      "val Loss: 1.7911 Acc: 0.1768\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 1.7909 Acc: 0.1738\n",
      "val Loss: 1.7912 Acc: 0.1774\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 1.7903 Acc: 0.1753\n",
      "val Loss: 1.7912 Acc: 0.1774\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 1.7906 Acc: 0.1816\n",
      "val Loss: 1.7912 Acc: 0.1761\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 1.7905 Acc: 0.1804\n",
      "val Loss: 1.7911 Acc: 0.1761\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 1.7903 Acc: 0.1835\n",
      "val Loss: 1.7911 Acc: 0.1734\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 1.7904 Acc: 0.1804\n",
      "val Loss: 1.7911 Acc: 0.1754\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 1.7903 Acc: 0.1806\n",
      "val Loss: 1.7911 Acc: 0.1720\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 1.7902 Acc: 0.1751\n",
      "val Loss: 1.7911 Acc: 0.1734\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 1.7902 Acc: 0.1774\n",
      "val Loss: 1.7910 Acc: 0.1693\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 1.7899 Acc: 0.1826\n",
      "val Loss: 1.7910 Acc: 0.1706\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 1.7902 Acc: 0.1770\n",
      "val Loss: 1.7911 Acc: 0.1693\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 1.7901 Acc: 0.1789\n",
      "val Loss: 1.7910 Acc: 0.1713\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 1.7897 Acc: 0.1830\n",
      "val Loss: 1.7910 Acc: 0.1713\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 1.7897 Acc: 0.1801\n",
      "val Loss: 1.7910 Acc: 0.1720\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 1.7893 Acc: 0.1847\n",
      "val Loss: 1.7910 Acc: 0.1672\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 1.7895 Acc: 0.1811\n",
      "val Loss: 1.7911 Acc: 0.1672\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 1.7894 Acc: 0.1814\n",
      "val Loss: 1.7911 Acc: 0.1679\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 1.7896 Acc: 0.1809\n",
      "val Loss: 1.7911 Acc: 0.1672\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 1.7903 Acc: 0.1791\n",
      "val Loss: 1.7911 Acc: 0.1666\n",
      "Training complete in 0m 21s\n",
      "Best val Acc: 0.186948\n",
      "[4 4 4 4 4]\n",
      "[[0.1473428  0.1473428  0.1473428  0.1473428  0.26328596 0.1473428 ]\n",
      " [0.14828402 0.14828402 0.1488165  0.15319706 0.22886156 0.17255682]\n",
      " [0.149092   0.149092   0.149092   0.149092   0.22606935 0.17756262]\n",
      " [0.14488457 0.14488457 0.15058607 0.15093774 0.26382247 0.14488457]\n",
      " [0.15484491 0.15484491 0.15484491 0.15484491 0.22577547 0.15484491]]\n"
     ]
    }
   ],
   "source": [
    "# Case 5. w/ data representation & fully-connected layers\n",
    "config = config5\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
    "print(prob[:5]) # shape : (2947, 6)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8c93687a9f3cac7ea1a38989caebc63561608f7a862e4f9a11f0ba4f68d9d9a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
