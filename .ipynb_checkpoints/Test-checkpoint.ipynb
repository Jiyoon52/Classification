{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9962b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import main_classificaiton as mc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
<<<<<<< HEAD
    "import random\n",
    "from sklearn.metrics import classification_report"
=======
<<<<<<< HEAD
    "import random\n",
    "from sklearn.metrics import classification_report"
=======
    "import random"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b8af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c4dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Case 1. w/o data representation & LSTM model \n",
    "config1 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'LSTM', # classification에에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC_layer'} 중 택 1\n",
=======
<<<<<<< HEAD
    "# Case 1. w/o data representation & LSTM model \n",
    "config1 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'LSTM', # classification에에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC_layer'} 중 택 1\n",
=======
    "# Case 1. w/o data representation & RNN model \n",
    "config1 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'RNN', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC_layer'} 중 택 1\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'num_layers' : 2, # recurrnet layers의 수, Default : 1\n",
    "            'hidden_size' : 64, # hidden state의 벡터차원 수\n",
    "            'attention' : False, # True일 경우 attention layer를 추가\n",
<<<<<<< HEAD
    "            'dropout' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
=======
<<<<<<< HEAD
    "            'dropout' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
=======
    "            'dropout' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "            'bidirectional' : True, # 모델의 양방향성 여부\n",
    "            'batch_size' : 64, #batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "# Case 2. w/o data representation & GRU model \n",
    "config2 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'GRU', # classification에에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC_layer'} 중 택 1\n",
=======
<<<<<<< HEAD
    "# Case 2. w/o data representation & GRU model \n",
    "config2 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'GRU', # classification에에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC_layer'} 중 택 1\n",
=======
    "# Case 2. w/o data representation &LSTM model \n",
    "config2 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'LSTM', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC_layer'} 중 택 1\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'num_layers' : 2, # recurrnet layers의 수, Default : 1\n",
    "            'hidden_size' : 64, # hidden state의 벡터차원 수\n",
    "            'attention' : False, # True일 경우 attention layer를 추가\n",
<<<<<<< HEAD
    "            'dropout' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
=======
<<<<<<< HEAD
    "            'dropout' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
=======
    "            'dropout' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "            'bidirectional' : True, # 모델의 양방향성 여부\n",
    "            'batch_size' : 64, #batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "# Case 3. w/o data representation & CNN_1D model \n",
=======
<<<<<<< HEAD
    "# Case 3. w/o data representation & CNN_1D model \n",
    "config3 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'CNN_1D', # classification에에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC_layer'} 중 택 1\n",
=======
    "# Case 3. w/o data representation & GRU model \n",
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "config3 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'CNN_1D', # classification에에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC_layer'} 중 택 1\n",
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
<<<<<<< HEAD
    "            'output_channels' : 64, # convolution channel size of output\n",
    "            'drop_out' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
=======
    "            'num_layers' : 2, # recurrnet layers의 수, Default : 1\n",
    "            'hidden_size' : 64, # hidden state의 벡터차원 수\n",
    "            'attention' : False, # True일 경우 attention layer를 추가\n",
    "            'dropout' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            'bidirectional' : True, # 모델의 양방향성 여부\n",
    "            'batch_size' : 64, #batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
    "# Case 4. w/o data representation & CNN_1D model \n",
    "config4 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'CNN_1D', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC_layer'} 중 택 1\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'output_channels' : 64, # convolution channel size of output\n",
<<<<<<< HEAD
    "            'drop_out' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
=======
    "            'drop_out' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "            'kernel_size' : 3, # convolutional filter size\n",
    "            'stride' : 1, # stride of the convolution. Default = 1 \n",
    "            'padding' : 0, # padding added to both sides of the input. Default = 0\n",
    "            'batch_size' : 64, # batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "# Case 4. w/o data representation & LSTM_FCNs model \n",
    "config4 = {\n",
    "        'with_representation': False, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'LSTM_FCNs', # classification에에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC_layer'} 중 택 1\n",
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
    "            'num_layers' : 1, # recurrnet layers의 수, Default : 1\n",
    "            'lstm_drop_out' : 0.4, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            'fc_drop_out' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "            'batch_size' : 256, # batch size\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}\n",
    "\n",
    "# Case 5. w data representation & fully-connected layers \n",
<<<<<<< HEAD
=======
=======
    "# Case 5. w data representation & fully-connected layers \n",
    "# 특징 벡터의 사이즈 = 20 이라고 가정\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "config5 = {\n",
    "        'with_representation': True, # classification에 사용되는 representation이 있을 경우 True, 아닐 경우 False\n",
    "        'model': 'FC', # classification에에 활용할 알고리즘 정의, {'RNN', 'LSTM', 'GRU', 'CNN_1D', 'FC'} 중 택 1\n",
    "\n",
    "        'parameter': {\n",
    "            'window_size' : 128, # input time series data를 windowing 하여 자르는 길이(size)\n",
<<<<<<< HEAD
    "            'drop_out' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
=======
<<<<<<< HEAD
    "            'drop_out' : 0.1, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
=======
    "            'input_representation' : 0, # 예를 들면 (80, 20) 차원의 벡터 (80은 window_length에 따른 관측치 수, 20은 representation 특징벡터 차원 수)를 넣어야 함. 지금은 loader부분에서 random값들어가 있음\n",
    "            'drop_out' : 0.2, # If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "            'batch_size' : 64, # batch size\n",
    "            'bias': True, # bias [True, False]\n",
    "            'device': 'cuda', # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'num_epochs' : 150 # 학습 시 사용할 epoch 수\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e84de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 9, 128)\n",
      "(7352,)\n",
      "(2947, 9, 128)\n",
      "(2947,)\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Raw data \n",
=======
<<<<<<< HEAD
    "# Raw data \n",
=======
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "data_dir = './data'\n",
    "\n",
    "train_x = pickle.load(open(os.path.join(data_dir, 'X_train.pkl'), 'rb'))\n",
    "train_y = pickle.load(open(os.path.join(data_dir, 'y_train.pkl'), 'rb'))\n",
    "test_x =  pickle.load(open(os.path.join(data_dir, 'X_test.pkl'), 'rb'))\n",
    "test_y = pickle.load(open(os.path.join(data_dir, 'y_test.pkl'), 'rb'))\n",
    "\n",
    "train_data = {'x' : train_x, 'y' : train_y}\n",
    "test_data = {'x' : test_x, 'y' : test_y}\n",
    "\n",
    "print(train_x.shape)  #shape : (num_of_instance x input_dims x window_size) = (7352, 9, 128)\n",
<<<<<<< HEAD
    "print(train_y.shape) #shape : (num_of_instance) = (7352, )\n",
    "print(test_x.shape)  #shape : (num_of_instance x input_dims x window_size) = (2947, 9, 128)\n",
    "print(test_y.shape)  #shape : (num_of_instance) = (2947, )"
=======
<<<<<<< HEAD
    "print(train_y.shape) #shape : (num_of_instance) = (7352, )\n",
    "print(test_x.shape)  #shape : (num_of_instance x input_dims x window_size) = (2947, 9, 128)\n",
    "print(test_y.shape)  #shape : (num_of_instance) = (2947, )"
=======
    "print(train_y.shape) #shape : (num_of_instance x input_dims x window_size) = (7352, )\n",
    "print(test_x.shape)  #shape : (num_of_instance x input_dims x window_size) = (2947, 9, 128)\n",
    "print(test_y.shape)  #shape : (num_of_instance x input_dims x window_size) = (2947)"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564b0770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
      "train Loss: 1.7789 Acc: 0.2663\n",
      "val Loss: 1.7555 Acc: 0.3569\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.0515 Acc: 0.5273\n",
      "val Loss: 1.1120 Acc: 0.5574\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.9011 Acc: 0.6179\n",
      "val Loss: 0.9829 Acc: 0.6071\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.7906 Acc: 0.6473\n",
      "val Loss: 0.7324 Acc: 0.6717\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.6235 Acc: 0.7009\n",
      "val Loss: 0.6675 Acc: 0.6540\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.5685 Acc: 0.7247\n",
      "val Loss: 0.6062 Acc: 0.7077\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.5548 Acc: 0.7473\n",
      "val Loss: 0.6014 Acc: 0.7077\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.5036 Acc: 0.7701\n",
      "val Loss: 0.5988 Acc: 0.7315\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.4671 Acc: 0.7961\n",
      "val Loss: 0.5340 Acc: 0.7668\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.4766 Acc: 0.7903\n",
      "val Loss: 0.5355 Acc: 0.7648\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.4500 Acc: 0.7966\n",
      "val Loss: 0.5546 Acc: 0.7709\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.4335 Acc: 0.8104\n",
      "val Loss: 0.5610 Acc: 0.7757\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.4243 Acc: 0.8150\n",
      "val Loss: 0.6111 Acc: 0.7736\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.4101 Acc: 0.8220\n",
      "val Loss: 0.4941 Acc: 0.8083\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.3858 Acc: 0.8339\n",
      "val Loss: 0.4822 Acc: 0.8185\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.3810 Acc: 0.8380\n",
      "val Loss: 0.4761 Acc: 0.8103\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.3500 Acc: 0.8488\n",
      "val Loss: 0.4839 Acc: 0.8137\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.3218 Acc: 0.8601\n",
      "val Loss: 0.4485 Acc: 0.8253\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.3052 Acc: 0.8704\n",
      "val Loss: 0.4361 Acc: 0.8362\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.2712 Acc: 0.8890\n",
      "val Loss: 0.3961 Acc: 0.8593\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.2477 Acc: 0.9058\n",
      "val Loss: 0.3794 Acc: 0.8790\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.2107 Acc: 0.9279\n",
      "val Loss: 0.3390 Acc: 0.9075\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.1688 Acc: 0.9383\n",
      "val Loss: 0.3230 Acc: 0.9177\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.1453 Acc: 0.9480\n",
      "val Loss: 0.2621 Acc: 0.9198\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.1281 Acc: 0.9520\n",
      "val Loss: 0.2608 Acc: 0.9320\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.1074 Acc: 0.9560\n",
      "val Loss: 0.2462 Acc: 0.9218\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.1059 Acc: 0.9543\n",
      "val Loss: 0.2440 Acc: 0.9130\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.0953 Acc: 0.9577\n",
      "val Loss: 0.2492 Acc: 0.9184\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0955 Acc: 0.9578\n",
      "val Loss: 0.2684 Acc: 0.9171\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0956 Acc: 0.9580\n",
      "val Loss: 0.2446 Acc: 0.9205\n",
      "Training complete in 4m 35s\n",
      "Best val Acc: 0.932019\n",
      "[4 4 4 4 4]\n",
      "[[3.1428826e-03 3.8197242e-05 1.9060582e-05 3.0262794e-03 9.9377233e-01\n",
      "  1.2182280e-06]\n",
      " [4.1432492e-03 4.3443866e-05 2.0092155e-05 2.5938053e-03 9.9319845e-01\n",
      "  9.1327803e-07]\n",
      " [3.7461284e-03 4.0831965e-05 1.9313755e-05 2.6819711e-03 9.9351078e-01\n",
      "  9.5121305e-07]\n",
      " [4.6638870e-03 4.5634115e-05 2.0469381e-05 2.4181218e-03 9.9285114e-01\n",
      "  7.9520919e-07]\n",
      " [4.0701623e-03 4.2379208e-05 1.9275158e-05 2.5371381e-03 9.9333018e-01\n",
      "  8.7419880e-07]]\n",
      "                  0.0         1.0         2.0         3.0         4.0  \\\n",
      "precision    0.900844    0.759398    0.807018    0.785311    0.878378   \n",
      "recall       0.860887    0.857749    0.876190    0.849287    0.733083   \n",
      "f1-score     0.880412    0.805583    0.840183    0.816047    0.799180   \n",
      "support    496.000000  471.000000  420.000000  491.000000  532.000000   \n",
      "\n",
      "                  5.0  accuracy    macro avg  weighted avg  \n",
      "precision    1.000000   0.85375     0.855158      0.859629  \n",
      "recall       0.949721   0.85375     0.854486      0.853750  \n",
      "f1-score     0.974212   0.85375     0.852603      0.854423  \n",
      "support    537.000000   0.85375  2947.000000   2947.000000  \n"
<<<<<<< HEAD
=======
=======
      "train Loss: 1.6584 Acc: 0.3578\n",
      "val Loss: 1.4538 Acc: 0.3759\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.0077 Acc: 0.5560\n",
      "val Loss: 1.0960 Acc: 0.5289\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.9814 Acc: 0.5594\n",
      "val Loss: 1.0072 Acc: 0.5595\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.9413 Acc: 0.5902\n",
      "val Loss: 0.9829 Acc: 0.5887\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.9057 Acc: 0.6235\n",
      "val Loss: 0.9704 Acc: 0.6159\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.6936 Acc: 0.7210\n",
      "val Loss: 0.7325 Acc: 0.7410\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.5424 Acc: 0.7854\n",
      "val Loss: 0.5766 Acc: 0.7954\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.4317 Acc: 0.8369\n",
      "val Loss: 0.4270 Acc: 0.8402\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.3776 Acc: 0.8636\n",
      "val Loss: 0.4722 Acc: 0.8600\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.3262 Acc: 0.8919\n",
      "val Loss: 0.4379 Acc: 0.8702\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.2836 Acc: 0.9102\n",
      "val Loss: 0.4246 Acc: 0.8872\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.2483 Acc: 0.9182\n",
      "val Loss: 0.3875 Acc: 0.9021\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.2417 Acc: 0.9197\n",
      "val Loss: 0.3927 Acc: 0.9035\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.2165 Acc: 0.9264\n",
      "val Loss: 0.3325 Acc: 0.8817\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.1973 Acc: 0.9362\n",
      "val Loss: 0.2994 Acc: 0.9191\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.1923 Acc: 0.9374\n",
      "val Loss: 0.2696 Acc: 0.9157\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.1789 Acc: 0.9396\n",
      "val Loss: 0.2829 Acc: 0.8973\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.2746 Acc: 0.9077\n",
      "val Loss: 0.2974 Acc: 0.9041\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.1718 Acc: 0.9405\n",
      "val Loss: 0.2879 Acc: 0.8953\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.1694 Acc: 0.9413\n",
      "val Loss: 0.3503 Acc: 0.9021\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.1770 Acc: 0.9379\n",
      "val Loss: 0.3379 Acc: 0.9075\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.1740 Acc: 0.9386\n",
      "val Loss: 0.4522 Acc: 0.8878\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.1743 Acc: 0.9446\n",
      "val Loss: 0.5574 Acc: 0.8572\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.1586 Acc: 0.9458\n",
      "val Loss: 0.2825 Acc: 0.9109\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.1545 Acc: 0.9473\n",
      "val Loss: 0.2687 Acc: 0.9075\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.1488 Acc: 0.9478\n",
      "val Loss: 0.2564 Acc: 0.9130\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.2603 Acc: 0.9155\n",
      "val Loss: 0.3338 Acc: 0.9048\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.2017 Acc: 0.9369\n",
      "val Loss: 0.3017 Acc: 0.9157\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.1846 Acc: 0.9432\n",
      "val Loss: 0.2603 Acc: 0.9157\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.7631 Acc: 0.8232\n",
      "val Loss: 0.3519 Acc: 0.8872\n",
      "Training complete in 3m 9s\n",
      "Best val Acc: 0.925221\n",
      "[4 4 4 4 4]\n",
      "[[ 3.0185163  -2.6709063  -1.3455262  -0.23610678  4.4431586  -8.567749  ]\n",
      " [ 0.43681976 -3.256823   -3.7350395   2.5898788   6.812459   -6.206282  ]\n",
      " [ 0.53687567 -3.2363987  -3.7682242   2.4976943   6.835698   -6.269879  ]\n",
      " [ 0.44587505 -3.299396   -3.7403483   2.5836961   6.904492   -6.277201  ]\n",
      " [ 0.4590189  -3.294892   -3.7132978   2.552673    6.872194   -6.281339  ]]\n"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Case 1. w/o data representation & LSTM\n",
=======
<<<<<<< HEAD
    "# Case 1. w/o data representation & LSTM\n",
=======
    "# Case 1. w/o data representation & RNN\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "config = config1\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
<<<<<<< HEAD
    "print(prob[:5]) # shape : (2947, 6)\n",
    "\n",
    "print(pd.DataFrame(classification_report(test_data['y'], pred, output_dict=True)))"
=======
<<<<<<< HEAD
    "print(prob[:5]) # shape : (2947, 6)\n",
    "\n",
    "print(pd.DataFrame(classification_report(test_data['y'], pred, output_dict=True)))"
=======
    "print(prob[:5]) # shape : (2947, 6)"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eefa80c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
      "train Loss: 1.7381 Acc: 0.2835\n",
      "val Loss: 1.6610 Acc: 0.3725\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.0349 Acc: 0.5535\n",
      "val Loss: 1.1303 Acc: 0.5670\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.7380 Acc: 0.7028\n",
      "val Loss: 0.7783 Acc: 0.6988\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.4793 Acc: 0.7851\n",
      "val Loss: 0.5477 Acc: 0.7879\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.4056 Acc: 0.8165\n",
      "val Loss: 0.5346 Acc: 0.7865\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.3649 Acc: 0.8427\n",
      "val Loss: 0.5201 Acc: 0.8239\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.3154 Acc: 0.8842\n",
      "val Loss: 0.4668 Acc: 0.8804\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.2150 Acc: 0.9359\n",
      "val Loss: 0.2604 Acc: 0.9354\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.1714 Acc: 0.9444\n",
      "val Loss: 0.2349 Acc: 0.9381\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.1570 Acc: 0.9434\n",
      "val Loss: 0.2270 Acc: 0.9395\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.1436 Acc: 0.9447\n",
      "val Loss: 0.2231 Acc: 0.9381\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.1376 Acc: 0.9463\n",
      "val Loss: 0.2210 Acc: 0.9388\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.1270 Acc: 0.9497\n",
      "val Loss: 0.2103 Acc: 0.9415\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.1179 Acc: 0.9519\n",
      "val Loss: 0.2121 Acc: 0.9381\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.1155 Acc: 0.9519\n",
      "val Loss: 0.2328 Acc: 0.9307\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.1156 Acc: 0.9510\n",
      "val Loss: 0.2119 Acc: 0.9409\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.1122 Acc: 0.9481\n",
      "val Loss: 0.2383 Acc: 0.9273\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.1033 Acc: 0.9554\n",
      "val Loss: 0.2104 Acc: 0.9402\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.1064 Acc: 0.9544\n",
      "val Loss: 0.2250 Acc: 0.9334\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.1008 Acc: 0.9572\n",
      "val Loss: 0.2205 Acc: 0.9368\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.0957 Acc: 0.9617\n",
      "val Loss: 0.2076 Acc: 0.9415\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.1027 Acc: 0.9570\n",
      "val Loss: 0.2142 Acc: 0.9381\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.0930 Acc: 0.9626\n",
      "val Loss: 0.2060 Acc: 0.9354\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.0988 Acc: 0.9582\n",
      "val Loss: 0.2097 Acc: 0.9327\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.0946 Acc: 0.9599\n",
      "val Loss: 0.2051 Acc: 0.9320\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.0914 Acc: 0.9619\n",
      "val Loss: 0.2087 Acc: 0.9388\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.0948 Acc: 0.9594\n",
      "val Loss: 0.2203 Acc: 0.9347\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.0899 Acc: 0.9626\n",
      "val Loss: 0.2163 Acc: 0.9361\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0883 Acc: 0.9641\n",
      "val Loss: 0.2337 Acc: 0.9307\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0989 Acc: 0.9568\n",
      "val Loss: 0.2288 Acc: 0.9313\n",
      "Training complete in 4m 4s\n",
      "Best val Acc: 0.942216\n",
      "[4 4 4 4 4]\n",
      "[[6.6147130e-03 4.3342070e-04 4.0883242e-06 3.1474165e-03 9.8979467e-01\n",
      "  5.6110716e-06]\n",
      " [4.1247495e-03 2.2937769e-04 2.0818159e-06 3.8583579e-03 9.9177963e-01\n",
      "  5.7735538e-06]\n",
      " [4.1023437e-03 2.3258195e-04 2.0553646e-06 3.8146349e-03 9.9184263e-01\n",
      "  5.8144237e-06]\n",
      " [4.1173976e-03 2.3404945e-04 2.0422199e-06 3.7382224e-03 9.9190253e-01\n",
      "  5.8004416e-06]\n",
      " [4.1938112e-03 2.4798021e-04 2.0607722e-06 3.5429583e-03 9.9200737e-01\n",
      "  5.8068881e-06]]\n",
      "                  0.0         1.0         2.0         3.0         4.0    5.0  \\\n",
      "precision    0.871940    0.940774    0.903073    0.824945    0.800000    1.0   \n",
      "recall       0.933468    0.876858    0.909524    0.767821    0.842105    1.0   \n",
      "f1-score     0.901655    0.907692    0.906287    0.795359    0.820513    1.0   \n",
      "support    496.000000  471.000000  420.000000  491.000000  532.000000  537.0   \n",
      "\n",
      "           accuracy    macro avg  weighted avg  \n",
      "precision   0.88904     0.890122      0.889897  \n",
      "recall      0.88904     0.888296      0.889040  \n",
      "f1-score    0.88904     0.888584      0.888842  \n",
      "support     0.88904  2947.000000   2947.000000  \n"
<<<<<<< HEAD
=======
=======
      "train Loss: 1.7821 Acc: 0.1978\n",
      "val Loss: 1.7576 Acc: 0.3297\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.0517 Acc: 0.5593\n",
      "val Loss: 1.1752 Acc: 0.5724\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.7946 Acc: 0.6781\n",
      "val Loss: 0.9321 Acc: 0.7247\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.7382 Acc: 0.7223\n",
      "val Loss: 0.8463 Acc: 0.7424\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.5938 Acc: 0.7769\n",
      "val Loss: 0.7406 Acc: 0.7587\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.5457 Acc: 0.7830\n",
      "val Loss: 0.6777 Acc: 0.7559\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.4712 Acc: 0.8152\n",
      "val Loss: 0.6655 Acc: 0.7791\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.4101 Acc: 0.8456\n",
      "val Loss: 0.6466 Acc: 0.7933\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.3734 Acc: 0.8546\n",
      "val Loss: 0.5965 Acc: 0.8226\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.3122 Acc: 0.8898\n",
      "val Loss: 0.5677 Acc: 0.8457\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.3138 Acc: 0.8919\n",
      "val Loss: 0.5175 Acc: 0.8654\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.2393 Acc: 0.9216\n",
      "val Loss: 0.4102 Acc: 0.8973\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.2519 Acc: 0.9192\n",
      "val Loss: 0.3028 Acc: 0.8973\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.2013 Acc: 0.9340\n",
      "val Loss: 0.2560 Acc: 0.9252\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.1814 Acc: 0.9401\n",
      "val Loss: 0.3428 Acc: 0.9137\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.1544 Acc: 0.9500\n",
      "val Loss: 0.2403 Acc: 0.9225\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.1428 Acc: 0.9507\n",
      "val Loss: 0.2599 Acc: 0.9239\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.1327 Acc: 0.9546\n",
      "val Loss: 0.3316 Acc: 0.9198\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.1520 Acc: 0.9503\n",
      "val Loss: 0.3697 Acc: 0.9103\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.1358 Acc: 0.9548\n",
      "val Loss: 0.3540 Acc: 0.9177\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.2023 Acc: 0.9347\n",
      "val Loss: 0.3751 Acc: 0.9177\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.1358 Acc: 0.9514\n",
      "val Loss: 0.2486 Acc: 0.9300\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.1135 Acc: 0.9561\n",
      "val Loss: 0.2405 Acc: 0.9307\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.1692 Acc: 0.9373\n",
      "val Loss: 0.2841 Acc: 0.9164\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.1088 Acc: 0.9570\n",
      "val Loss: 0.2559 Acc: 0.9286\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.1059 Acc: 0.9604\n",
      "val Loss: 0.2313 Acc: 0.9334\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.1033 Acc: 0.9599\n",
      "val Loss: 0.2243 Acc: 0.9293\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.1072 Acc: 0.9583\n",
      "val Loss: 0.3012 Acc: 0.8994\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0918 Acc: 0.9626\n",
      "val Loss: 0.2494 Acc: 0.9245\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0946 Acc: 0.9621\n",
      "val Loss: 0.2750 Acc: 0.9184\n",
      "Training complete in 3m 17s\n",
      "Best val Acc: 0.936778\n",
      "[4 4 4 4 4]\n",
      "[[-0.04416232 -1.5564657  -2.6390538   0.23135655  6.2844687  -6.211173  ]\n",
      " [-0.13038693 -1.6591768  -2.58128     0.29581937  6.2984447  -6.2150154 ]\n",
      " [-0.1676458  -1.7086916  -2.5640073   0.33197075  6.3000836  -6.2027993 ]\n",
      " [-0.16678253 -1.7073661  -2.5630195   0.33032888  6.30126    -6.206491  ]\n",
      " [-0.12793724 -1.6679486  -2.5685513   0.28996184  6.2983913  -6.212824  ]]\n"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Case 2. w/o data representation & GRU\n",
=======
<<<<<<< HEAD
    "# Case 2. w/o data representation & GRU\n",
=======
    "# Case 2. w/o data representation & LSTM\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "config = config2\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
<<<<<<< HEAD
    "print(prob[:5]) # shape : (2947, 6)\n",
    "\n",
    "print(pd.DataFrame(classification_report(test_data['y'], pred, output_dict=True)))"
=======
<<<<<<< HEAD
    "print(prob[:5]) # shape : (2947, 6)\n",
    "\n",
    "print(pd.DataFrame(classification_report(test_data['y'], pred, output_dict=True)))"
=======
    "print(prob[:5]) # shape : (2947, 6)"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7674f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
      "train Loss: 1.6921 Acc: 0.4606\n",
      "val Loss: 1.5258 Acc: 0.6220\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 0.5038 Acc: 0.8153\n",
      "val Loss: 0.6735 Acc: 0.8035\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.3418 Acc: 0.8772\n",
      "val Loss: 0.6024 Acc: 0.8389\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.2777 Acc: 0.8997\n",
      "val Loss: 0.5746 Acc: 0.8559\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.2316 Acc: 0.9199\n",
      "val Loss: 0.5430 Acc: 0.8749\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.1975 Acc: 0.9306\n",
      "val Loss: 0.5114 Acc: 0.8939\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.1694 Acc: 0.9381\n",
      "val Loss: 0.4864 Acc: 0.9028\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.1462 Acc: 0.9486\n",
      "val Loss: 0.4714 Acc: 0.9028\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.1299 Acc: 0.9534\n",
      "val Loss: 0.4587 Acc: 0.9082\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.1189 Acc: 0.9560\n",
      "val Loss: 0.4640 Acc: 0.9055\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.1088 Acc: 0.9575\n",
      "val Loss: 0.4569 Acc: 0.9096\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.1012 Acc: 0.9585\n",
      "val Loss: 0.4401 Acc: 0.9116\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.0946 Acc: 0.9597\n",
      "val Loss: 0.4293 Acc: 0.9109\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.0899 Acc: 0.9611\n",
      "val Loss: 0.4102 Acc: 0.9109\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.0862 Acc: 0.9624\n",
      "val Loss: 0.4008 Acc: 0.9109\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.0839 Acc: 0.9640\n",
      "val Loss: 0.3955 Acc: 0.9082\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.0802 Acc: 0.9631\n",
      "val Loss: 0.3775 Acc: 0.9123\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.0795 Acc: 0.9648\n",
      "val Loss: 0.3650 Acc: 0.9130\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.0753 Acc: 0.9657\n",
      "val Loss: 0.3571 Acc: 0.9164\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.0736 Acc: 0.9657\n",
      "val Loss: 0.3620 Acc: 0.9171\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.0698 Acc: 0.9675\n",
      "val Loss: 0.3570 Acc: 0.9239\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.0691 Acc: 0.9662\n",
      "val Loss: 0.3395 Acc: 0.9252\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.0683 Acc: 0.9670\n",
      "val Loss: 0.3499 Acc: 0.9239\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.0669 Acc: 0.9699\n",
      "val Loss: 0.3529 Acc: 0.9259\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.0666 Acc: 0.9692\n",
      "val Loss: 0.3517 Acc: 0.9245\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.0630 Acc: 0.9682\n",
      "val Loss: 0.3559 Acc: 0.9252\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.0612 Acc: 0.9709\n",
      "val Loss: 0.3608 Acc: 0.9266\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.0595 Acc: 0.9725\n",
      "val Loss: 0.3711 Acc: 0.9232\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0586 Acc: 0.9716\n",
      "val Loss: 0.3824 Acc: 0.9252\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0569 Acc: 0.9730\n",
      "val Loss: 0.3881 Acc: 0.9266\n",
      "Training complete in 0m 44s\n",
      "Best val Acc: 0.926581\n",
      "[4 4 4 4 4]\n",
      "[[1.3835733e-04 2.0991228e-04 1.1575056e-10 1.6110012e-04 9.9949062e-01\n",
      "  4.3692094e-12]\n",
      " [1.7275727e-04 1.2295740e-04 5.7558597e-11 1.2112593e-04 9.9958318e-01\n",
      "  1.3214138e-12]\n",
      " [5.2806929e-05 1.9448133e-04 2.5666153e-11 4.5264318e-05 9.9970740e-01\n",
      "  9.2810276e-13]\n",
      " [2.3459537e-05 2.5745551e-04 8.7912611e-12 3.0766307e-05 9.9968827e-01\n",
      "  8.3606901e-13]\n",
      " [9.8980026e-06 1.1235011e-04 1.7613328e-12 2.4049179e-05 9.9985373e-01\n",
      "  2.5974405e-13]]\n",
      "                  0.0         1.0         2.0         3.0         4.0  \\\n",
      "precision    0.965517    0.833969    0.879386    0.850103    0.895257   \n",
      "recall       0.903226    0.927813    0.954762    0.843177    0.851504   \n",
      "f1-score     0.933333    0.878392    0.915525    0.846626    0.872832   \n",
      "support    496.000000  471.000000  420.000000  491.000000  532.000000   \n",
      "\n",
      "                  5.0  accuracy    macro avg  weighted avg  \n",
      "precision    1.000000  0.903631     0.904039      0.906588  \n",
      "recall       0.949721  0.903631     0.905034      0.903631  \n",
      "f1-score     0.974212  0.903631     0.903487      0.904095  \n",
      "support    537.000000  0.903631  2947.000000   2947.000000  \n"
<<<<<<< HEAD
=======
=======
      "train Loss: 1.7403 Acc: 0.3537\n",
      "val Loss: 1.6700 Acc: 0.4555\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 0.9985 Acc: 0.5676\n",
      "val Loss: 1.0738 Acc: 0.5540\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.8697 Acc: 0.6382\n",
      "val Loss: 0.8905 Acc: 0.6540\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.4917 Acc: 0.8536\n",
      "val Loss: 0.4752 Acc: 0.8858\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.2645 Acc: 0.9277\n",
      "val Loss: 0.3380 Acc: 0.9021\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.1948 Acc: 0.9408\n",
      "val Loss: 0.2771 Acc: 0.9245\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.1731 Acc: 0.9388\n",
      "val Loss: 0.2624 Acc: 0.9293\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.1524 Acc: 0.9456\n",
      "val Loss: 0.2625 Acc: 0.9300\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.1442 Acc: 0.9464\n",
      "val Loss: 0.2558 Acc: 0.9307\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.1337 Acc: 0.9488\n",
      "val Loss: 0.2497 Acc: 0.9361\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.1266 Acc: 0.9519\n",
      "val Loss: 0.2436 Acc: 0.9341\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.1228 Acc: 0.9527\n",
      "val Loss: 0.2503 Acc: 0.9320\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.1244 Acc: 0.9512\n",
      "val Loss: 0.2425 Acc: 0.9341\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.1145 Acc: 0.9566\n",
      "val Loss: 0.2410 Acc: 0.9375\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.1163 Acc: 0.9527\n",
      "val Loss: 0.2477 Acc: 0.9341\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.1115 Acc: 0.9531\n",
      "val Loss: 0.2394 Acc: 0.9361\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.1145 Acc: 0.9515\n",
      "val Loss: 0.2444 Acc: 0.9327\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.1048 Acc: 0.9558\n",
      "val Loss: 0.2421 Acc: 0.9327\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.1034 Acc: 0.9551\n",
      "val Loss: 0.2421 Acc: 0.9327\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.1025 Acc: 0.9566\n",
      "val Loss: 0.2421 Acc: 0.9341\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.0990 Acc: 0.9585\n",
      "val Loss: 0.2459 Acc: 0.9334\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.1042 Acc: 0.9551\n",
      "val Loss: 0.2447 Acc: 0.9341\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.0996 Acc: 0.9577\n",
      "val Loss: 0.2457 Acc: 0.9225\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.0971 Acc: 0.9590\n",
      "val Loss: 0.2304 Acc: 0.9381\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.0988 Acc: 0.9575\n",
      "val Loss: 0.2310 Acc: 0.9375\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.0958 Acc: 0.9597\n",
      "val Loss: 0.2485 Acc: 0.9334\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.0990 Acc: 0.9604\n",
      "val Loss: 0.2376 Acc: 0.9388\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.0932 Acc: 0.9616\n",
      "val Loss: 0.2362 Acc: 0.9347\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0974 Acc: 0.9606\n",
      "val Loss: 0.2371 Acc: 0.9361\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0936 Acc: 0.9604\n",
      "val Loss: 0.2476 Acc: 0.9334\n",
      "Training complete in 3m 8s\n",
      "Best val Acc: 0.940857\n",
      "[4 4 4 4 4]\n",
      "[[ 0.90820694 -1.5440568  -8.312525    0.61867917  6.9758263  -7.0844517 ]\n",
      " [ 0.872182   -1.5198749  -8.271869    0.6334086   6.942082   -7.0175853 ]\n",
      " [ 0.87774587 -1.5120534  -8.248257    0.61669725  6.9415245  -7.0360622 ]\n",
      " [ 0.88496083 -1.5213611  -8.2281      0.6151486   6.94689    -7.0753155 ]\n",
      " [ 0.8851025  -1.5039475  -8.24159     0.60359526  6.9388146  -7.032606  ]]\n"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Case 3. w/o data representation & CNN_1D\n",
=======
<<<<<<< HEAD
    "# Case 3. w/o data representation & CNN_1D\n",
=======
    "# Case 3. w/o data representation & GRU\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "config = config3\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
<<<<<<< HEAD
    "print(prob[:5]) # shape : (2947, 6)\n",
    "\n",
    "print(pd.DataFrame(classification_report(test_data['y'], pred, output_dict=True)))"
=======
<<<<<<< HEAD
    "print(prob[:5]) # shape : (2947, 6)\n",
    "\n",
    "print(pd.DataFrame(classification_report(test_data['y'], pred, output_dict=True)))"
=======
    "print(prob[:5]) # shape : (2947, 6)"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59275e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
      "train Loss: 1.6852 Acc: 0.4542\n",
      "val Loss: 1.7355 Acc: 0.5656\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.3337 Acc: 0.6819\n",
      "val Loss: 1.3285 Acc: 0.6567\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.9891 Acc: 0.8062\n",
      "val Loss: 0.9974 Acc: 0.7927\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.8252 Acc: 0.9223\n",
      "val Loss: 0.8648 Acc: 0.8953\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.6896 Acc: 0.9415\n",
      "val Loss: 0.7679 Acc: 0.9007\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.5904 Acc: 0.9546\n",
      "val Loss: 0.6917 Acc: 0.9028\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.5156 Acc: 0.9594\n",
      "val Loss: 0.6275 Acc: 0.9041\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.4533 Acc: 0.9619\n",
      "val Loss: 0.5726 Acc: 0.9048\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.3985 Acc: 0.9636\n",
      "val Loss: 0.5319 Acc: 0.9048\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.3537 Acc: 0.9638\n",
      "val Loss: 0.4924 Acc: 0.9055\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.3157 Acc: 0.9653\n",
      "val Loss: 0.4623 Acc: 0.9041\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.2811 Acc: 0.9662\n",
      "val Loss: 0.4415 Acc: 0.9048\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.2518 Acc: 0.9657\n",
      "val Loss: 0.4205 Acc: 0.9109\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.2277 Acc: 0.9663\n",
      "val Loss: 0.4053 Acc: 0.9123\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.2063 Acc: 0.9689\n",
      "val Loss: 0.3944 Acc: 0.9055\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.1851 Acc: 0.9694\n",
      "val Loss: 0.3769 Acc: 0.9055\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.1673 Acc: 0.9711\n",
      "val Loss: 0.3709 Acc: 0.9150\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.1527 Acc: 0.9728\n",
      "val Loss: 0.3486 Acc: 0.9164\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.1367 Acc: 0.9755\n",
      "val Loss: 0.3437 Acc: 0.9062\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.1245 Acc: 0.9745\n",
      "val Loss: 0.3472 Acc: 0.9150\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.1119 Acc: 0.9755\n",
      "val Loss: 0.3311 Acc: 0.9171\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.1052 Acc: 0.9760\n",
      "val Loss: 0.3166 Acc: 0.9130\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.0958 Acc: 0.9782\n",
      "val Loss: 0.3069 Acc: 0.9177\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.0925 Acc: 0.9770\n",
      "val Loss: 0.3151 Acc: 0.9177\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.0847 Acc: 0.9781\n",
      "val Loss: 0.3031 Acc: 0.9198\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.0868 Acc: 0.9793\n",
      "val Loss: 0.3029 Acc: 0.9211\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.0811 Acc: 0.9772\n",
      "val Loss: 0.3113 Acc: 0.9123\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.0707 Acc: 0.9811\n",
      "val Loss: 0.2875 Acc: 0.9252\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0707 Acc: 0.9784\n",
      "val Loss: 0.3027 Acc: 0.9062\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0613 Acc: 0.9842\n",
      "val Loss: 0.3015 Acc: 0.9157\n",
      "Training complete in 1m 33s\n",
      "Best val Acc: 0.932019\n",
      "[4 4 4 4 4]\n",
      "[[6.5858681e-03 3.0865753e-03 8.2912314e-04 9.1310067e-04 9.8856550e-01\n",
      "  1.9814664e-05]\n",
      " [1.5846780e-02 5.7159988e-03 1.8494719e-03 1.9806575e-03 9.7458148e-01\n",
      "  2.5617033e-05]\n",
      " [1.9886993e-02 7.2369594e-03 2.1636828e-03 2.8627983e-03 9.6782368e-01\n",
      "  2.5834570e-05]\n",
      " [1.8312940e-02 7.7453386e-03 1.7938456e-03 2.2900952e-03 9.6983778e-01\n",
      "  1.9999021e-05]\n",
      " [1.5127777e-02 6.5489658e-03 1.5352112e-03 1.6374437e-03 9.7513264e-01\n",
      "  1.7915218e-05]]\n",
      "                  0.0         1.0         2.0         3.0         4.0  \\\n",
      "precision    0.950397    0.971554    0.931264    0.875294    0.820000   \n",
      "recall       0.965726    0.942675    1.000000    0.757637    0.924812   \n",
      "f1-score     0.958000    0.956897    0.964409    0.812227    0.869258   \n",
      "support    496.000000  471.000000  420.000000  491.000000  532.000000   \n",
      "\n",
      "                  5.0  accuracy    macro avg  weighted avg  \n",
      "precision    1.000000  0.921955     0.924751      0.924038  \n",
      "recall       0.949721  0.921955     0.923429      0.921955  \n",
      "f1-score     0.974212  0.921955     0.922500      0.921384  \n",
      "support    537.000000  0.921955  2947.000000   2947.000000  \n"
<<<<<<< HEAD
=======
=======
      "train Loss: 1.5987 Acc: 0.5013\n",
      "val Loss: 1.3670 Acc: 0.6084\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 0.5261 Acc: 0.8019\n",
      "val Loss: 0.6510 Acc: 0.7797\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.3782 Acc: 0.8662\n",
      "val Loss: 0.5126 Acc: 0.8239\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.2891 Acc: 0.9051\n",
      "val Loss: 0.4063 Acc: 0.8817\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.2326 Acc: 0.9254\n",
      "val Loss: 0.3486 Acc: 0.8926\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.1978 Acc: 0.9345\n",
      "val Loss: 0.3156 Acc: 0.8967\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.1759 Acc: 0.9415\n",
      "val Loss: 0.2921 Acc: 0.9014\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.1598 Acc: 0.9452\n",
      "val Loss: 0.2803 Acc: 0.9055\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.1453 Acc: 0.9495\n",
      "val Loss: 0.2741 Acc: 0.9007\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.1322 Acc: 0.9527\n",
      "val Loss: 0.2675 Acc: 0.9123\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.1259 Acc: 0.9539\n",
      "val Loss: 0.2670 Acc: 0.9198\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.1179 Acc: 0.9560\n",
      "val Loss: 0.2752 Acc: 0.9225\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.1124 Acc: 0.9544\n",
      "val Loss: 0.2663 Acc: 0.9218\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.1077 Acc: 0.9563\n",
      "val Loss: 0.2838 Acc: 0.9055\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.1014 Acc: 0.9561\n",
      "val Loss: 0.2826 Acc: 0.9130\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.0972 Acc: 0.9594\n",
      "val Loss: 0.2958 Acc: 0.9096\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.0936 Acc: 0.9616\n",
      "val Loss: 0.3022 Acc: 0.9103\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.0908 Acc: 0.9614\n",
      "val Loss: 0.2979 Acc: 0.9103\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.0877 Acc: 0.9624\n",
      "val Loss: 0.3101 Acc: 0.9109\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.0846 Acc: 0.9648\n",
      "val Loss: 0.3149 Acc: 0.9116\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.0825 Acc: 0.9645\n",
      "val Loss: 0.3291 Acc: 0.9109\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.0793 Acc: 0.9651\n",
      "val Loss: 0.3333 Acc: 0.9116\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.0780 Acc: 0.9658\n",
      "val Loss: 0.3464 Acc: 0.9130\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.0763 Acc: 0.9658\n",
      "val Loss: 0.3428 Acc: 0.9123\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.0756 Acc: 0.9672\n",
      "val Loss: 0.3695 Acc: 0.9109\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.0710 Acc: 0.9682\n",
      "val Loss: 0.3517 Acc: 0.9109\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.0694 Acc: 0.9674\n",
      "val Loss: 0.3726 Acc: 0.9123\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.0683 Acc: 0.9696\n",
      "val Loss: 0.3947 Acc: 0.9123\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.0641 Acc: 0.9699\n",
      "val Loss: 0.3882 Acc: 0.9123\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.0634 Acc: 0.9719\n",
      "val Loss: 0.3761 Acc: 0.9123\n",
      "Training complete in 0m 46s\n",
      "Best val Acc: 0.922502\n",
      "[4 4 4 4 4]\n",
      "[[ -0.0545887   -0.8739962  -10.135557     0.318584     6.984183\n",
      "  -14.561371  ]\n",
      " [ -0.33102724  -1.252699   -10.63112      2.2375767    7.984505\n",
      "  -14.874614  ]\n",
      " [ -0.65144193  -0.7991334  -10.766919     2.4478502    8.478192\n",
      "  -15.0501375 ]\n",
      " [ -0.6649623   -0.51520693 -10.887545     2.5151854    8.7294035\n",
      "  -15.23084   ]\n",
      " [ -0.63186735  -0.34990746 -11.049662     2.696743     9.054423\n",
      "  -15.535     ]]\n"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Case 4. w/o data representation & LSTM_FCNs\n",
=======
<<<<<<< HEAD
    "# Case 4. w/o data representation & LSTM_FCNs\n",
=======
    "# Case 4. w/o data representation & CNN_1D\n",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "config = config4\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "print(prob[:5]) # shape : (2947, 6)\n",
    "\n",
    "print(pd.DataFrame(classification_report(test_data['y'], pred, output_dict=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1a60a",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
<<<<<<< HEAD
=======
=======
    "print(prob[:5]) # shape : (2947, 6)"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
   "id": "3085234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128)\n",
      "(7352,)\n",
      "(2947, 128)\n",
      "(2947,)\n"
     ]
    }
   ],
   "source": [
    "# Representation data\n",
    "data_dir = './data'\n",
    "\n",
    "train_x = pd.read_csv(os.path.join(data_dir, 'ts2vec_repr_train.csv'))\n",
    "train_y = pickle.load(open(os.path.join(data_dir, 'y_train.pkl'), 'rb'))\n",
    "\n",
    "test_x = pd.read_csv(os.path.join(data_dir, 'ts2vec_repr_test.csv'))\n",
    "test_y = pickle.load(open(os.path.join(data_dir, 'y_test.pkl'), 'rb'))\n",
    "\n",
    "train_data = {'x' : train_x, 'y' : train_y}\n",
    "test_data = {'x' : test_x, 'y' : test_y}\n",
    "\n",
    "print(train_x.shape)  #shape : (num_of_instance x representation_dims) = (7352, 9, 128)\n",
    "print(train_y.shape) #shape : (num_of_instance) = (7352, )\n",
    "print(test_x.shape)  #shape : (num_of_instance x representation_dims) = (2947, 9, 128)\n",
    "print(test_y.shape)  #shape : (num_of_instance) = (2947, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
<<<<<<< HEAD
=======
=======
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
   "id": "b370aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
      "train Loss: 1.6923 Acc: 0.3856\n",
      "val Loss: 1.6072 Acc: 0.5350\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 0.9423 Acc: 0.6948\n",
      "val Loss: 0.8945 Acc: 0.7410\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 0.6747 Acc: 0.7614\n",
      "val Loss: 0.6682 Acc: 0.7791\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 0.5527 Acc: 0.8062\n",
      "val Loss: 0.5651 Acc: 0.8178\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 0.4724 Acc: 0.8357\n",
      "val Loss: 0.4980 Acc: 0.8321\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 0.4103 Acc: 0.8563\n",
      "val Loss: 0.4440 Acc: 0.8484\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 0.3651 Acc: 0.8786\n",
      "val Loss: 0.3966 Acc: 0.8790\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 0.3195 Acc: 0.8941\n",
      "val Loss: 0.3584 Acc: 0.8960\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 0.2863 Acc: 0.9044\n",
      "val Loss: 0.3313 Acc: 0.8939\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 0.2542 Acc: 0.9182\n",
      "val Loss: 0.3054 Acc: 0.9041\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 0.2301 Acc: 0.9281\n",
      "val Loss: 0.2858 Acc: 0.9123\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 0.2122 Acc: 0.9320\n",
      "val Loss: 0.2721 Acc: 0.9164\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 0.1963 Acc: 0.9398\n",
      "val Loss: 0.2596 Acc: 0.9184\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 0.1876 Acc: 0.9384\n",
      "val Loss: 0.2486 Acc: 0.9225\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 0.1703 Acc: 0.9464\n",
      "val Loss: 0.2429 Acc: 0.9218\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 0.1627 Acc: 0.9478\n",
      "val Loss: 0.2337 Acc: 0.9239\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 0.1557 Acc: 0.9478\n",
      "val Loss: 0.2272 Acc: 0.9259\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 0.1484 Acc: 0.9500\n",
      "val Loss: 0.2224 Acc: 0.9320\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 0.1426 Acc: 0.9498\n",
      "val Loss: 0.2209 Acc: 0.9266\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 0.1376 Acc: 0.9522\n",
      "val Loss: 0.2160 Acc: 0.9327\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 0.1312 Acc: 0.9565\n",
      "val Loss: 0.2100 Acc: 0.9347\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 0.1305 Acc: 0.9543\n",
      "val Loss: 0.2079 Acc: 0.9347\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 0.1281 Acc: 0.9519\n",
      "val Loss: 0.2054 Acc: 0.9354\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 0.1251 Acc: 0.9539\n",
      "val Loss: 0.2061 Acc: 0.9361\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 0.1196 Acc: 0.9558\n",
      "val Loss: 0.2033 Acc: 0.9375\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 0.1166 Acc: 0.9549\n",
      "val Loss: 0.2014 Acc: 0.9354\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 0.1142 Acc: 0.9565\n",
      "val Loss: 0.2035 Acc: 0.9361\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 0.1124 Acc: 0.9580\n",
      "val Loss: 0.2005 Acc: 0.9375\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 0.1097 Acc: 0.9594\n",
      "val Loss: 0.2008 Acc: 0.9375\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 0.1108 Acc: 0.9561\n",
      "val Loss: 0.1993 Acc: 0.9375\n",
      "Training complete in 0m 25s\n",
      "Best val Acc: 0.938137\n",
      "[4 4 4 4 4]\n",
      "[[4.3652311e-05 3.2642618e-04 1.5995928e-06 6.6966298e-03 9.9292678e-01\n",
      "  4.8134893e-06]\n",
      " [4.7802228e-06 6.5756125e-05 3.8763639e-07 7.6732626e-03 9.9225473e-01\n",
      "  1.0959986e-06]\n",
      " [2.0228617e-06 4.7563150e-05 3.4184202e-07 9.7589809e-03 9.9019021e-01\n",
      "  9.4529292e-07]\n",
      " [2.6051623e-06 5.3089814e-05 3.5020051e-07 8.0245109e-03 9.9191850e-01\n",
      "  1.0092582e-06]\n",
      " [2.4374933e-06 5.9906775e-05 3.8326880e-07 7.8893155e-03 9.9204683e-01\n",
      "  1.0995636e-06]]\n",
      "                  0.0         1.0         2.0         3.0         4.0    5.0  \\\n",
      "precision    0.952965    0.960465    0.868922    0.828125    0.794737    1.0   \n",
      "recall       0.939516    0.876858    0.978571    0.755601    0.851504    1.0   \n",
      "f1-score     0.946193    0.916759    0.920493    0.790202    0.822142    1.0   \n",
      "support    496.000000  471.000000  420.000000  491.000000  532.000000  537.0   \n",
      "\n",
      "           accuracy    macro avg  weighted avg  \n",
      "precision  0.899559     0.900869      0.901393  \n",
      "recall     0.899559     0.900342      0.899559  \n",
      "f1-score   0.899559     0.899298      0.899247  \n",
      "support    0.899559  2947.000000   2947.000000  \n"
<<<<<<< HEAD
=======
=======
      "train Loss: 1.8038 Acc: 0.1717\n",
      "val Loss: 1.7895 Acc: 0.1937\n",
      "\n",
      "Epoch 6/150\n",
      "train Loss: 1.7943 Acc: 0.1777\n",
      "val Loss: 1.7870 Acc: 0.1971\n",
      "\n",
      "Epoch 11/150\n",
      "train Loss: 1.7936 Acc: 0.1750\n",
      "val Loss: 1.7873 Acc: 0.2039\n",
      "\n",
      "Epoch 16/150\n",
      "train Loss: 1.7923 Acc: 0.1753\n",
      "val Loss: 1.7874 Acc: 0.2060\n",
      "\n",
      "Epoch 21/150\n",
      "train Loss: 1.7918 Acc: 0.1785\n",
      "val Loss: 1.7874 Acc: 0.2060\n",
      "\n",
      "Epoch 26/150\n",
      "train Loss: 1.7925 Acc: 0.1755\n",
      "val Loss: 1.7873 Acc: 0.2026\n",
      "\n",
      "Epoch 31/150\n",
      "train Loss: 1.7925 Acc: 0.1741\n",
      "val Loss: 1.7873 Acc: 0.2053\n",
      "\n",
      "Epoch 36/150\n",
      "train Loss: 1.7912 Acc: 0.1831\n",
      "val Loss: 1.7873 Acc: 0.2046\n",
      "\n",
      "Epoch 41/150\n",
      "train Loss: 1.7910 Acc: 0.1738\n",
      "val Loss: 1.7873 Acc: 0.2046\n",
      "\n",
      "Epoch 46/150\n",
      "train Loss: 1.7912 Acc: 0.1728\n",
      "val Loss: 1.7873 Acc: 0.2039\n",
      "\n",
      "Epoch 51/150\n",
      "train Loss: 1.7908 Acc: 0.1811\n",
      "val Loss: 1.7872 Acc: 0.2005\n",
      "\n",
      "Epoch 56/150\n",
      "train Loss: 1.7913 Acc: 0.1799\n",
      "val Loss: 1.7873 Acc: 0.2053\n",
      "\n",
      "Epoch 61/150\n",
      "train Loss: 1.7910 Acc: 0.1794\n",
      "val Loss: 1.7874 Acc: 0.2012\n",
      "\n",
      "Epoch 66/150\n",
      "train Loss: 1.7903 Acc: 0.1850\n",
      "val Loss: 1.7873 Acc: 0.2005\n",
      "\n",
      "Epoch 71/150\n",
      "train Loss: 1.7899 Acc: 0.1887\n",
      "val Loss: 1.7873 Acc: 0.2019\n",
      "\n",
      "Epoch 76/150\n",
      "train Loss: 1.7894 Acc: 0.1903\n",
      "val Loss: 1.7874 Acc: 0.1999\n",
      "\n",
      "Epoch 81/150\n",
      "train Loss: 1.7906 Acc: 0.1796\n",
      "val Loss: 1.7875 Acc: 0.1992\n",
      "\n",
      "Epoch 86/150\n",
      "train Loss: 1.7891 Acc: 0.1870\n",
      "val Loss: 1.7875 Acc: 0.1999\n",
      "\n",
      "Epoch 91/150\n",
      "train Loss: 1.7902 Acc: 0.1823\n",
      "val Loss: 1.7874 Acc: 0.1971\n",
      "\n",
      "Epoch 96/150\n",
      "train Loss: 1.7905 Acc: 0.1847\n",
      "val Loss: 1.7875 Acc: 0.1944\n",
      "\n",
      "Epoch 101/150\n",
      "train Loss: 1.7893 Acc: 0.1857\n",
      "val Loss: 1.7875 Acc: 0.1944\n",
      "\n",
      "Epoch 106/150\n",
      "train Loss: 1.7888 Acc: 0.1853\n",
      "val Loss: 1.7876 Acc: 0.1971\n",
      "\n",
      "Epoch 111/150\n",
      "train Loss: 1.7888 Acc: 0.1876\n",
      "val Loss: 1.7876 Acc: 0.1958\n",
      "\n",
      "Epoch 116/150\n",
      "train Loss: 1.7900 Acc: 0.1806\n",
      "val Loss: 1.7877 Acc: 0.1924\n",
      "\n",
      "Epoch 121/150\n",
      "train Loss: 1.7888 Acc: 0.1876\n",
      "val Loss: 1.7878 Acc: 0.1903\n",
      "\n",
      "Epoch 126/150\n",
      "train Loss: 1.7893 Acc: 0.1867\n",
      "val Loss: 1.7878 Acc: 0.1897\n",
      "\n",
      "Epoch 131/150\n",
      "train Loss: 1.7885 Acc: 0.1884\n",
      "val Loss: 1.7878 Acc: 0.1883\n",
      "\n",
      "Epoch 136/150\n",
      "train Loss: 1.7892 Acc: 0.1855\n",
      "val Loss: 1.7879 Acc: 0.1876\n",
      "\n",
      "Epoch 141/150\n",
      "train Loss: 1.7882 Acc: 0.1889\n",
      "val Loss: 1.7880 Acc: 0.1897\n",
      "\n",
      "Epoch 146/150\n",
      "train Loss: 1.7886 Acc: 0.1842\n",
      "val Loss: 1.7880 Acc: 0.1897\n",
      "Training complete in 0m 21s\n",
      "Best val Acc: 0.209381\n",
      "[4 4 4 0 4]\n",
      "[[0.         0.         0.         0.         0.20967467 0.11200541]\n",
      " [0.         0.         0.         0.         0.09573306 0.05175306]\n",
      " [0.05794232 0.         0.         0.         0.0742238  0.        ]\n",
      " [0.07968983 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.05837218 0.0251669 ]]\n"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
     ]
    }
   ],
   "source": [
    "# Case 5. w/ data representation & fully-connected layers\n",
    "config = config5\n",
    "data_classification = mc.Classification(config, train_data, test_data)\n",
    "pred, prob = data_classification.getResult()\n",
    "\n",
    "# test_loader : shuffle = False \n",
    "print(pred[:5]) # shape : (2947, )\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
    "print(prob[:5]) # shape : (2947, 6)\n",
    "\n",
    "print(pd.DataFrame(classification_report(test_data['y'], pred, output_dict=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43781a",
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
=======
=======
    "print(prob[:5]) # shape : (2947, 6)"
   ]
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
>>>>>>> 360090b9b0d0ee87ffcf77271712b958eba53481
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8c93687a9f3cac7ea1a38989caebc63561608f7a862e4f9a11f0ba4f68d9d9a"
  },
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3 (ipykernel)",
=======
   "display_name": "Python 3",
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.4"
=======
   "version": "3.7.6"
>>>>>>> d07f8f2287b498aaaec882ee2bd4ea0095340b9b
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
